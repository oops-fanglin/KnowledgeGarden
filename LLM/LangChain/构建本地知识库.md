---
title: 构建本地知识库
date created: 2024-06-22 17:22
date modified: 2024-06-22 17:22
dg-publish: true
---


总的来讲最基础的流程有以下几步：文本读取/分割、文本向量化、向量化存储、langchain检索向量并构造prompt喂给大模型

文本读取、分割
langchain_community.**document_loaders**中提供了很丰富的接口，可以解析csv、html、json、markdown、pdf等多种格式。

```python
from langchain_community.document_loaders import TextLoader
loader = TextLoader("textForLangchain/仿生人会梦见电子羊吗.txt")
loader.encoding = "utf-8"
textResult = loader.load()
```

接下来再对加载的文本进行文本分割。
``` python
from langchain.text_splitter import RecursiveCharacterTextSplitter
textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
```
里面有两个参数，一个是chunk_size，代表一个文本块的最大长度，一个是chunk_overlap，代表文本块之间最大的重叠长度，可以在一定程度上保持文本的连续性。
分割块的长度可能主要取决于大模型支持的最大上下文长度
## 文本向量化存储

文本向量化即为将一段文本转化为一个向量，这里需要用到一个embedding模型。Embedding模型是一种将文本、图片等数据映射到低维空间，并用向量表示的一种手段，对于大模型里的文本检索可以说是相当重要
``` python
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
embedModelName = "../bge-large-zh-v1.5"
embedModel = HuggingFaceEmbeddings(model_name=embedModelName)
vectorDB = FAISS.from_texts(texts=documents, embeddings=embedModel)
retriever = vectorDB.as_retriever()
```

## 调用大模型
到这里就已经可以调用大模型了，不是我太跳跃，而是langchain封装的接口太方便。当我们获取到检索器后，就可以通过langchain的RetrievalQA接口对大模型进行调用。

``` python
from langchain_community.llms.chatglm3 import ChatGLM3
from langchain.chains import RetrievalQA
llm = ChatGLM3(model="THUDM\chatglm3-6b")
qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
rs = qa_chain.invoke("德卡德是谁")
```
 

参考资料
[基于langchain和chatglm3构建本地知识库学习（一）](https://blog.csdn.net/tdlmcpo/article/details/136786214)